{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "D8E6ihbqj1ah"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "from os import listdir\n",
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fHVy8vjVCcAJ"
   },
   "outputs": [],
   "source": [
    "# list of files are read from the directory location\n",
    "\n",
    "def getListOfFiles(dirName):\n",
    "    listOfFile = os.listdir(dirName)\n",
    "    count=0\n",
    "    arr_num=[]\n",
    "    allFiles = list()\n",
    "    for entry in listOfFile:\n",
    "        wholePath = os.path.join(dirName, entry)\n",
    "        count+=1\n",
    "        flag= True\n",
    "        # got one entry\n",
    "        \n",
    "        if os.path.isdir(wholePath):\n",
    "            allFiles = allFiles + getListOfFiles(wholePath)\n",
    "            continue\n",
    "        \n",
    "        allFiles.append(wholePath)\n",
    "        arr_num.append(count)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduces the text to lower case\n",
    "def to_lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_punct(token):     #https://www.geeksforgeeks.org/python-remove-punctuation-from-string/\n",
    "    \n",
    "    punc= set(string.punctuation)\n",
    "    for t in token:\n",
    "        if t in punc:\n",
    "            token.remove(t)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "k4Cx8IaUDhDV",
    "outputId": "b9183bd0-8ece-4255-efe5-ce7ac6dc88eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84558\n"
     ]
    }
   ],
   "source": [
    "# dataset is in the same directory as the code file \n",
    "path1=\"./Humor,Hist,Media,Food\"\n",
    "\n",
    "# tokens is the positional index data structure which is used to retrieve data \n",
    "tokens={}\n",
    "f_path=[]\n",
    "\n",
    "input_n= 0\n",
    "total_doc_id=[]\n",
    "\n",
    "# the file path is taken\n",
    "f_path=getListOfFiles(path1)\n",
    "\n",
    "i=0\n",
    "while i< len(f_path):\n",
    "    \n",
    "    encod= 'utf-8'\n",
    "    err_= 'ignore'\n",
    "    text=open(f_path[i],encoding=encod ,errors=err_).read()\n",
    "    \n",
    "    # a)\n",
    "    text= to_lower(text)\n",
    "        \n",
    "    # b)\n",
    "    tokenizer= TweetTokenizer()\n",
    "    token =tokenizer.tokenize(text)\n",
    "    \n",
    "    # c) stop words   -https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "    stop_words= set(stopwords.words('english'))\n",
    "    token = [word for word in token if not to_lower(word) in stop_words]\n",
    "    \n",
    "    # d) punctuations\n",
    "    token = remove_punct(token)  \n",
    "    \n",
    "    # e) remove space\n",
    "    for t in token:\n",
    "        tem= t\n",
    "        x= tem.strip()\n",
    "        if x==\"\" or x==\" \":\n",
    "            token.remove(t)\n",
    "\n",
    "    input_n+=1\n",
    "    \n",
    "    index =f_path[i]\n",
    "    index= index.rfind('/')\n",
    "    doc_id=f_path[i][index+1:len(f_path[i])]\n",
    "    total_doc_id.append(doc_id)\n",
    "    \n",
    "    # after preprocessing the text we append the data keywords along with the document ids as shown\n",
    "    tok= token\n",
    "    for count,name in enumerate(tok):\n",
    "        if name in tokens:\n",
    "            tokens[name][0]=tokens[name][0]+1\n",
    "            if doc_id in tokens[name][1]:\n",
    "                flag1= True  #reached\n",
    "                tokens[name][1][doc_id].append(count)\n",
    "                continue\n",
    "            tokens[name][1][doc_id]=[count]\n",
    "            \n",
    "        else:\n",
    "            tokens[name]=[]\n",
    "            en= 1\n",
    "            tokens[name].append(en)\n",
    "            tokens[name].append({})\n",
    "            input_n+=1\n",
    "            tokens[name][1][doc_id]=[count]\n",
    "\n",
    "    i+=1\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_query(query_tok, pos):\n",
    "    i=0\n",
    "    query_tok.pop(pos)\n",
    "    return query_tok\n",
    "\n",
    "# function to tokenize\n",
    "def tokenizer_it(text):\n",
    "    tokenizer= TweetTokenizer()\n",
    "    token =tokenizer.tokenize(text)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_ind(matches, query_tok):\n",
    "    matched_docs = []\n",
    "    \n",
    "    for match in matches:\n",
    "        doc = match[0]\n",
    "        pos = match[1]\n",
    "        \n",
    "        tuple_mat= (match[0], pos)\n",
    "        count = 0\n",
    "        \n",
    "        # iteration on the query token is performed\n",
    "        indd= 0\n",
    "        while indd< len(query_tok):\n",
    "            k= query_tok[indd]\n",
    "            \n",
    "            pos = pos+1\n",
    "            \n",
    "            index_no= 0\n",
    "            index_no+=1\n",
    "            k_pos = tokens[k][index_no]\n",
    "            \n",
    "            if index_no == 0:\n",
    "                k_pos = tokens[k][index_no]\n",
    "            docs_list = [z for z in k_pos]\n",
    "            \n",
    "            # if the document is in list it is appended or added in the doc positions\n",
    "            if doc in docs_list:\n",
    "                \n",
    "                doc_positions = {}\n",
    "                #------get positions\n",
    "                for posting_value in k_pos:\n",
    "        \n",
    "                    if posting_value == doc:\n",
    "                        doc_positions= k_pos[posting_value]\n",
    "                \n",
    "                #------\n",
    "                got_doc= 0\n",
    "                if pos in doc_positions:\n",
    "                    got_doc+=index_no\n",
    "                \n",
    "                # the position is incremented after each iteration\n",
    "                if pos in doc_positions:\n",
    "                    count += 1\n",
    "                else:\n",
    "                    count += 1\n",
    "                    got_doc-=1\n",
    "                    break\n",
    "            \n",
    "            len_of_tok= len(query_tok)\n",
    "            indd+=1\n",
    "            \n",
    "            # at the end file name is appended in the list\n",
    "            if count == len_of_tok:\n",
    "                \n",
    "                doc_namee= match[0]\n",
    "                temp= doc_namee.split(\"\\\\\")\n",
    "                matched_docs.append(temp[1])\n",
    "                \n",
    "\n",
    "    to_ret= set(matched_docs)\n",
    "    \n",
    "    return to_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_single_query(query_tok, query_in):\n",
    "    total_docs= 0\n",
    "    docs_contain=[]\n",
    "    flag= True\n",
    "    \n",
    "    ind= 0\n",
    "    \n",
    "    query_tok= query_tok[0]\n",
    "    if query_tok in tokens:\n",
    "        \n",
    "        list_word_postingss= list(tokens[query_tok][1])\n",
    "        first_item=0\n",
    "        \n",
    "        # at the end file name is appended in the list\n",
    "        while ind< len(tokens[query_tok][1]):\n",
    "            a= list_word_postingss[ind]\n",
    "            temp= a.split(\"\\\\\")\n",
    "            ind+=1\n",
    "            docs_contain.append(temp[1])\n",
    "            total_docs+=1\n",
    "        \n",
    "    print(\"The number of documents retrieved: \", total_docs)\n",
    "    print(\"List of document names retrieved: \",docs_contain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART C\n",
    "\n",
    "def get_query_ans(query_in):\n",
    "\n",
    "    text= query_in\n",
    "    #preprocessing query\n",
    "    # a)\n",
    "    text= to_lower(text)\n",
    "\n",
    "    # b) tokenize\n",
    "    token =tokenizer_it(text)\n",
    "\n",
    "    # c) stop words   -https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
    "    stop_words= set(stopwords.words('english'))\n",
    "    token = [word for word in token if not to_lower(word) in stop_words]\n",
    "\n",
    "    # d) punctuations\n",
    "    token = remove_punct(token)  \n",
    "\n",
    "    # e) remove space\n",
    "    for t in token:\n",
    "        tem= t\n",
    "        x= tem.strip()\n",
    "        if x==\"\" or x==\" \":\n",
    "            token.remove(t)\n",
    "\n",
    "    query_tok= token\n",
    "    \n",
    "    total_docs= 0\n",
    "    docs_contain=[]\n",
    "    if len(query_tok)==1:\n",
    "        \n",
    "        len_single_query(query_tok, query_in)\n",
    "        return\n",
    "    \n",
    "    null_item= False\n",
    "    if len(query_tok)==0:\n",
    "        print(\"The number of documents retrieved: \", 0)\n",
    "        print(\"List of document names retrieved: []\",)\n",
    "        return \n",
    "    in_word = query_tok[0]\n",
    "    \n",
    "    \n",
    "    #------ get initiale item\n",
    "    \n",
    "    matches = []\n",
    "    poss= []\n",
    "    word_postings= {}\n",
    "    \n",
    "    if in_word not in tokens:\n",
    "        word_postings= {}\n",
    "    else: word_postings = tokens[in_word][1]\n",
    "        \n",
    "    \n",
    "    ind= 0\n",
    "    list_word_postingss= list(word_postings.keys())\n",
    "    first_item=0\n",
    "    \n",
    "    # iteration on the posting list is done to append positions\n",
    "    while ind< len(word_postings):\n",
    "        word_posting= list_word_postingss[ind]\n",
    "        \n",
    "        if(len(matches)== 0):\n",
    "            null_item= True\n",
    "        for positions in tokens[in_word][1][word_posting]:\n",
    "            matches.append((word_posting, positions))\n",
    "            if ind!=0:\n",
    "                first_item =1\n",
    "            poss.append(positions)\n",
    "            \n",
    "        ind+=1\n",
    "    \n",
    "    #------\n",
    "    \n",
    "    query_tok= pop_query(query_tok, 0)\n",
    "#     print(matches[0])\n",
    "    mat = iter(matches)\n",
    "    res_dct =dict( zip (mat,mat))\n",
    "    \n",
    "    total_matched_docs = get_positional_ind(matches, query_tok)\n",
    "    print(\"The number of documents retrieved: \", len(total_matched_docs))\n",
    "    print(\"List of document names retrieved: \", total_matched_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of documents retrieved:  5\n",
      "List of document names retrieved:  {'eatme.txt', 'drinkrul.jok', 'beerwarn.txt', 'anime.cli', 'wkrp.epi'}\n"
     ]
    }
   ],
   "source": [
    "# query_in= input(\"Enter the search phrase query:\")\n",
    "query_in= \"big guy .\"\n",
    "get_query_ans(query_in)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "IR_Q2_Assign1",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
